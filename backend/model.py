import torch
import cv2
import os
import numpy as np
from facenet_pytorch import MTCNN
from efficientnet_pytorch import EfficientNet
from skimage.metrics import structural_similarity as ssim
import torchvision.transforms as transforms
from scipy.special import expit

from sqlalchemy import create_engine, Column, Integer, String, Boolean, TIMESTAMP
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

# âœ… ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì„¤ì •
MODEL_DIR = "models"
os.makedirs(MODEL_DIR, exist_ok=True)

# (ì¶”ê°€) DB MySQL ì—°ê²°
DATABASE_URL = "mysql+pymysql://root:12345678@localhost/video_db"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#DBì— ì €ì¥
engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(bind=engine, autocommit=False, autoflush=False)

Base = declarative_base()

class VideoVote(Base):
    __tablename__ = "video_votes"

    id = Column(Integer, primary_key=True, index=True, autoincrement=True)
    video_id = Column(String(255), nullable=False)
    user_id = Column(String(255), nullable=False)
    vote = Column(Boolean, nullable=False)
    created_at = Column(TIMESTAMP, server_default="CURRENT_TIMESTAMP")

Base.metadata.create_all(bind=engine)


# âœ… **BlazeFace ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (MTCNN ëŒ€ì²´)**
face_detector = MTCNN(keep_all=True, device=device)

# âœ… **EfficientNet ëª¨ë¸ ë¡œë“œ**
net = EfficientNet.from_pretrained('efficientnet-b4').to(device)
net.eval()

# âœ… **ë°ì´í„° ì „ì²˜ë¦¬ ì„¤ì •**
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Resize((224, 224)),
    transforms.Normalize(mean=[0.5], std=[0.5])  # ì •ê·œí™” ì¶”ê°€
])


# âœ… ì–¼êµ´ ê°ì§€ (MTCNN ì‚¬ìš©)
def detect_faces(video_path, frame_step=10):
    cap = cv2.VideoCapture(video_path)
    face_detected = False
    frame_count = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret or frame_count > 100:
            break

        if frame_count % frame_step == 0:
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            faces, _ = face_detector.detect(rgb_frame)
            if faces is not None and len(faces) > 0:
                face_detected = True
                break

        frame_count += 1

    cap.release()
    return face_detected

# âœ… ì–¼êµ´ì´ ìˆëŠ” ê²½ìš° AI íƒì§€ (EfficientNet)
def detect_fake_face(video_path, frame_step=15):
    cap = cv2.VideoCapture(video_path)
    face_scores = []
    frame_count = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret or frame_count > 100:
            break

        if frame_count % frame_step == 0:
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            faces, _ = face_detector.detect(rgb_frame)

            if faces is not None and len(faces) > 0:
                x1, y1, x2, y2 = map(int, faces[0])

                # âœ… ì–¼êµ´ í¬ê¸° ê²€ì¦ (0ì´ê±°ë‚˜ ìŒìˆ˜ë©´ ê¸°ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ)
                if x2 - x1 <= 0 or y2 - y1 <= 0:
                    print(f"ğŸš¨ ì˜ëª»ëœ ì–¼êµ´ í¬ê¸° ê°ì§€ (x1={x1}, y1={y1}, x2={x2}, y2={y2}) â†’ ê¸°ë³¸ í¬ê¸° ì‚¬ìš©")
                    face = cv2.resize(rgb_frame, (224, 224))
                else:
                    face = rgb_frame[y1:y2, x1:x2]

                # âœ… í¬ë¡­ëœ ì–¼êµ´ì´ ì •ìƒì ì¸ì§€ ì¶”ê°€ ì²´í¬
                if face.shape[0] == 0 or face.shape[1] == 0:
                    print("ğŸš¨ ì–¼êµ´ í¬ë¡­ ì‹¤íŒ¨ â†’ ê¸°ë³¸ í¬ê¸° ì‚¬ìš©")
                    face = cv2.resize(rgb_frame, (224, 224))

                # âœ… ë³€í™˜ í›„ ëª¨ë¸ ì…ë ¥
                face_tensor = transform(face).unsqueeze(0).to(device)

                with torch.no_grad():
                    score = net(face_tensor).cpu().numpy().flatten()
                    face_scores.append(expit(score[0]))

        frame_count += 1

    cap.release()
    return np.mean(face_scores) if face_scores else 0.5

# âœ… ì–¼êµ´ì´ ì—†ëŠ” ê²½ìš° AI íƒì§€ (Optical Flow + Edge Map)
def detect_fake_no_face(video_path, frame_step=10):
    cap = cv2.VideoCapture(video_path)
    frame_count = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret or frame_count > 100:
            break

        if frame_count % frame_step == 0:
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            faces, _ = face_detector.detect(rgb_frame)

            if faces is not None and len(faces) > 0:
                x1, y1, x2, y2 = map(int, faces[0])
                print(f"âœ… ê°ì§€ëœ ì–¼êµ´ ì¢Œí‘œ: x1={x1}, y1={y1}, x2={x2}, y2={y2}")

                if x2 - x1 > 0 and y2 - y1 > 0:
                    cap.release()
                    return True

        frame_count += 1

    cap.release()
    return False


# âœ… **ìµœì¢… ì‹¤í–‰ í•¨ìˆ˜**
def predict_video(video_path):
    print(f"ğŸ” ë¶„ì„ ì‹œì‘: {video_path}")

    if detect_faces(video_path):
        print("ğŸ”µ ì–¼êµ´ ê°ì§€ë¨ â†’ ì–¼êµ´ ê¸°ë°˜ AI íƒì§€")
        score = detect_fake_face(video_path)
    else:
        print("ğŸŸ  ì–¼êµ´ ì—†ìŒ â†’ ë¹„ì–¼êµ´ AI íƒì§€")
        score = detect_fake_no_face(video_path)

    fscore = 1 - score
    print(f"ğŸ”¹ Score for real video: {score:.6f}")
    print(f"ğŸ”¹ Score for fake video: {fscore:.6f}")

    result = {}
    if score > 0.5:
        print("âœ… ì´ ì˜ìƒì€ **REAL** ì…ë‹ˆë‹¤.")
        result["message"] = "âœ… ì´ ì˜ìƒì€ **REAL** ì…ë‹ˆë‹¤."
    else:
        print("ğŸš¨ ì´ ì˜ìƒì€ **FAKE** ì…ë‹ˆë‹¤!")
        result["message"] = "ğŸš¨ ì´ ì˜ìƒì€ **FAKE** ì…ë‹ˆë‹¤!"

    result["real_score"] = float(score)
    result["fake_score"] = float(fscore)

    return result

